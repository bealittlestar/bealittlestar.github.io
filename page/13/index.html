<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://flytowardnewworld.github.io/page/13/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://flytowardnewworld.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-deep-learning-ai深度学习笔记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time class="dt-published" datetime="2019-04-11T01:15:23.000Z" itemprop="datePublished">2019-04-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E3%81%AE%E5%8E%86%E7%A8%8B-Journal-of-Studying/">学习の历程(Journal of Studying)</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">deep_learning.ai深度学习笔记&lt;Andrew Ng&gt;</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id=""><a href="#" class="headerlink" title=""></a><!-- more  --></h1><h1 id="C5-Sequence-Models"><a href="#C5-Sequence-Models" class="headerlink" title="C5: Sequence Models"></a>C5: Sequence Models</h1><h2 id="W1-Recurrent-Neural-Networks-循环序列模型"><a href="#W1-Recurrent-Neural-Networks-循环序列模型" class="headerlink" title="W1 : Recurrent Neural Networks (循环序列模型)"></a>W1 : Recurrent Neural Networks (循环序列模型)</h2><h3 id="L1-：-Why-Sequence-Models"><a href="#L1-：-Why-Sequence-Models" class="headerlink" title="L1 ： Why Sequence Models?"></a>L1 ： Why Sequence Models?</h3><p>循环神经网络（<strong>RNN</strong>）之类的模型在语音识别、自然语言处理和其他领域中引起变革。</p>
<p>序列模型的列子</p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc11.png"></p>
<h3 id="L2-Notation-数学符号"><a href="#L2-Notation-数学符号" class="headerlink" title="L2 : Notation 数学符号"></a>L2 : Notation 数学符号</h3><p>NLP</p>
<p>我们用$X^{(i)}$来表示第个i训练样本，所以为了指代第个t元素，或者说是训练样本i的序列中第t个元素用$X^{(i)}<t>$这个符号来表示。如果是序列长度$T_x$，那么你的训练集里不同的训练样本就会有不同的长度，所以$T_x^{(i)}$就代表第个训练样本的输入序列长度。同样$y^{(i)}<t>$代表第i个训练样本中第t个元素，$T_y^{(i)}$就是第i个训练样本的输出序列的长度。</p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc12.png"></p>
<p>预先有一个词典</p>
<h3 id="L3-Recurrent-Neural-Network-Model-循环神经网络模型"><a href="#L3-Recurrent-Neural-Network-Model-循环神经网络模型" class="headerlink" title="L3 : Recurrent Neural Network Model (循环神经网络模型)"></a>L3 : Recurrent Neural Network Model (循环神经网络模型)</h3><p>现在我们讨论一下怎样才能建立一个模型，建立一个神经网络来学习X到Y的映射</p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc13.png"></p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc14.png"></p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc15.png"></p>
<p>$a^{&lt;0&gt;}$通常 是零向量</p>
<p>N模型包含三类权重系数，分别是Wax，Waa，Wya。且不同元素之间同一位置共享同一权重系数。</p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc16.png"></p>
<p>RNN的正向传播（Forward Propagation）过程为：</p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc17.png"></p>
<p>循环神经网络用的激活函数经常是<strong>tanh</strong>，不过有时候也会用<strong>ReLU</strong>，但是<strong>tanh</strong>是更通常的选择，我们有其他方法来避免梯度消失问题，我们将在之后进行讲述。选用哪个激活函数是取决于你的输出y，如果它是一个二分问题，那么我猜你会用<strong>sigmoid</strong>函数作为激活函数，如果是k类别分类问题的话，那么可以选用<strong>softmax</strong>作为激活函数。不过这里激活函数的类型取决于你有什么样类型的输出y，对于命名实体识别来说y只可能是0或者1，那我猜这里第二个激活函数g可以是<strong>sigmoid</strong>激活函数。</p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc18.png"></p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc19.png"></p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc110.png"></p>
<h3 id="c4-Backpropagation-through-time-通过时间的反向传播"><a href="#c4-Backpropagation-through-time-通过时间的反向传播" class="headerlink" title="c4: Backpropagation through time ( 通过时间的反向传播)"></a>c4: Backpropagation through time ( 通过时间的反向传播)</h3><p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc111.png"></p>
<p>参数的关系<img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc112.png">*</p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc113.png"></p>
<p>单个元素的Loss function:</p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc114.png"></p>
<p>该样本所有元素的Loss function为：</p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc115.png"></p>
<p>然后，反向传播（Backpropagation）过程就是从右到左分别计算L(y^,y)对参数Wa，Wy，ba，by的偏导数。思路与做法与标准的神经网络是一样的。一般可以通过成熟的深度学习框架自动求导，例如PyTorch、Tensorflow等。这种从右到左的求导过程被称为Backpropagation through time</p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc116.png"></p>
<h3 id="L5-Different-types-of-RNNs-不同类型的循环神经网络"><a href="#L5-Different-types-of-RNNs-不同类型的循环神经网络" class="headerlink" title="L5: Different types of RNNs (不同类型的循环神经网络)"></a>L5: Different types of <strong>RNN</strong>s (不同类型的循环神经网络)</h3><p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc117.png"></p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc118.png"></p>
<p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc119.png"></p>
<h3 id="L6-Language-model-and-sequence-generation-语言模型和序列生成"><a href="#L6-Language-model-and-sequence-generation-语言模型和序列生成" class="headerlink" title="L6 : Language model and sequence generation (语言模型和序列生成)"></a>L6 : Language model and sequence generation (语言模型和序列生成)</h3><p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc120.png"></p>
<h3 id="L7-Sampling-novel-sequences-对新序列采样"><a href="#L7-Sampling-novel-sequences-对新序列采样" class="headerlink" title="L7 : Sampling novel sequences (对新序列采样)"></a>L7 : Sampling novel sequences (对新序列采样)</h3><p><img src="/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc121.png"></p>
<h3 id="Vanishing-gradients-with-RNNs-循环神经网络的梯度消失"><a href="#Vanishing-gradients-with-RNNs-循环神经网络的梯度消失" class="headerlink" title="Vanishing gradients with RNNs (循环神经网络的梯度消失)"></a>Vanishing gradients with <strong>RNN</strong>s (循环神经网络的梯度消失)</h3><p>首先从左到右前向传播，然后反向传播。但是反向传播会很困难，因为同样的梯度消失的问题，后面层的输出误差（上图编号6所示）很难影响前面层（上图编号7所示的层）的计算。这就意味着，实际上很难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的<strong>was</strong>或者<strong>were</strong>。而且在英语里面，这中间的内容（上图编号8所示）可以任意长，对吧？所以你需要长时间记住单词是单数还是复数，这样后面的句子才能用到这些信息。也正是这个原因，所以基本的<strong>RNN</strong>模型会有很多局部影响</p>
<p><a target="_blank" rel="noopener" href="http://www.ai-start.com/dl2017/html/lesson5-week1.html">http://www.ai-start.com/dl2017/html/lesson5-week1.html</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484029&amp;idx=1&amp;sn=c93b5eddec33dc29dc172a5ea0d76822&amp;chksm=976fa7e0a0182ef61e36d1c32aa0706c4e81e1762a7ee2554165beecde929b72cf026c5b7a64&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484029&amp;idx=1&amp;sn=c93b5eddec33dc29dc172a5ea0d76822&amp;chksm=976fa7e0a0182ef61e36d1c32aa0706c4e81e1762a7ee2554165beecde929b72cf026c5b7a64&amp;scene=21#wechat_redirect</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://flytowardnewworld.github.io/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-id="cl5v7wh60001vi0vkfbz18m71" data-title="deep_learning.ai深度学习笔记&lt;Andrew Ng&gt;" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/" rel="tag">deep learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%88%91%E7%9A%84%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="tag">我的读书笔记</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-deeplearningvideo" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/04/03/deeplearningvideo/" class="article-date">
  <time class="dt-published" datetime="2019-04-03T02:19:55.000Z" itemprop="datePublished">2019-04-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E8%A7%86%E9%A2%91%E5%AD%A6%E4%B9%A0/">视频学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/04/03/deeplearningvideo/">deeplearningvideo</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><strong>Coursera深度学习教程中文笔记</strong></p>
<!-- more  -->

<p>课程概述</p>
<p><a target="_blank" rel="noopener" href="https://mooc.study.163.com/university/deeplearning_ai#/c">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p>
<p>这些课程专为已有一定基础（基本的编程知识，熟悉<strong>Python</strong>、对机器学习有基本了解），想要尝试进入人工智能领域的计算机专业人士准备。介绍显示：“深度学习是科技业最热门的技能之一，本课程将帮你掌握深度学习。”</p>
<p>在这5堂课中，学生将可以学习到深度学习的基础，学会构建神经网络，并用在包括吴恩达本人在内的多位业界顶尖专家指导下创建自己的机器学习项目。<strong>Deep Learning Specialization</strong>对卷积神经网络 (<strong>CNN</strong>)、递归神经网络 (<strong>RNN</strong>)、长短期记忆 (<strong>LSTM</strong>) 等深度学习常用的网络结构、工具和知识都有涉及。</p>
<p><strong>笔记是根据视频和字幕写的，没有技术含量，只需要专注和严谨。</strong></p>
<p>2018-04-14</p>
<p><strong>本课程视频教程地址：</strong><a target="_blank" rel="noopener" href="https://mooc.study.163.com/university/deeplearning_ai#/c">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p>
<p>（该视频从<a target="_blank" rel="noopener" href="http://www.deeplearning.ai/">www.deeplearning.ai</a> 网站下载，因众所周知的原因，国内用户观看某些在线视频非常不容易，故一些学者一起制作了离线视频，旨在方便国内用户个人学习使用，请勿用于商业用途。视频内嵌中英文字幕，推荐使用<strong>potplayer</strong>播放。版权属于吴恩达老师所有，若在线视频流畅，请到官方网站观看。）</p>
<p><a target="_blank" rel="noopener" href="http://www.ai-start.com/">笔记网站(适合手机阅读)</a></p>
<p>吴恩达老师的机器学习课程笔记和视频：<a target="_blank" rel="noopener" href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p>
<h1 id="深度学习笔记目录"><a href="#深度学习笔记目录" class="headerlink" title="深度学习笔记目录"></a>深度学习笔记目录</h1><h2 id="第一门课-神经网络和深度学习-Neural-Networks-and-Deep-Learning"><a href="#第一门课-神经网络和深度学习-Neural-Networks-and-Deep-Learning" class="headerlink" title="第一门课 神经网络和深度学习(Neural Networks and Deep Learning)"></a>第一门课 神经网络和深度学习(Neural Networks and Deep Learning)</h2><p>第一周：深度学习引言(Introduction to Deep Learning)</p>
<p>1.1 欢迎(Welcome)</p>
<p>1.2 什么是神经网络？(What is a Neural Network)</p>
<p>1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</p>
<p>1.4 为什么神经网络会流行？(Why is Deep Learning taking off?)</p>
<p>1.5 关于本课程(About this Course)</p>
<p>1.6 课程资源(Course Resources)</p>
<p>1.7 Geoffery Hinton 专访(Geoffery Hinton interview)</p>
<p>第二周：神经网络的编程基础(Basics of Neural Network programming)</p>
<p>2.1 二分类(Binary Classification)</p>
<p>2.2 逻辑回归(Logistic Regression)</p>
<p>2.3 逻辑回归的代价函数（Logistic Regression Cost Function）</p>
<p>2.4 梯度下降（Gradient Descent）</p>
<p>2.5 导数（Derivatives）</p>
<p>2.6 更多的导数例子（More Derivative Examples）</p>
<p>2.7 计算图（Computation Graph）</p>
<p>2.8 计算图导数（Derivatives with a Computation Graph）</p>
<p>2.9 逻辑回归的梯度下降（Logistic Regression Gradient Descent）</p>
<p>2.10 梯度下降的例子(Gradient Descent on m Examples)</p>
<p>2.11 向量化(Vectorization)</p>
<p>2.12 更多的向量化例子（More Examples of Vectorization）</p>
<p>2.13 向量化逻辑回归(Vectorizing Logistic Regression)</p>
<p>2.14 向量化逻辑回归的梯度计算（Vectorizing Logistic Regression’s Gradient）</p>
<p>2.15 Python中的广播机制（Broadcasting in Python）</p>
<p>2.16 关于 Python与numpy向量的使用（A note on python or numpy vectors）</p>
<p>2.17 Jupyter&#x2F;iPython Notebooks快速入门（Quick tour of Jupyter&#x2F;iPython Notebooks）</p>
<p>2.18 逻辑回归损失函数详解（Explanation of logistic regression cost function）</p>
<p>第三周：浅层神经网络(Shallow neural networks)</p>
<p>3.1 神经网络概述（Neural Network Overview）</p>
<p>3.2 神经网络的表示（Neural Network Representation）</p>
<p>3.3 计算一个神经网络的输出（Computing a Neural Network’s output）</p>
<p>3.4 多样本向量化（Vectorizing across multiple examples）</p>
<p>3.5 向量化实现的解释（Justification for vectorized implementation）</p>
<p>3.6 激活函数（Activation functions）</p>
<p>3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）</p>
<p>3.8 激活函数的导数（Derivatives of activation functions）</p>
<p>3.9 神经网络的梯度下降（Gradient descent for neural networks）</p>
<p>3.10（选修）直观理解反向传播（Backpropagation intuition）</p>
<p>3.11 随机初始化（Random+Initialization）</p>
<p>第四周：深层神经网络(Deep Neural Networks)</p>
<p>4.1 深层神经网络（Deep L-layer neural network）</p>
<p>4.2 前向传播和反向传播（Forward and backward propagation）</p>
<p>4.3 深层网络中的前向和反向传播（Forward propagation in a Deep Network）</p>
<p>4.4 核对矩阵的维数（Getting your matrix dimensions right）</p>
<p>4.5 为什么使用深层表示？（Why deep representations?）</p>
<p>4.6 搭建神经网络块（Building blocks of deep neural networks）</p>
<p>4.7 参数VS超参数（Parameters vs Hyperparameters）</p>
<p>4.8 深度学习和大脑的关联性（What does this have to do with the brain?）</p>
<h2 id="第二门课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization"><a href="#第二门课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization" class="headerlink" title="第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)"></a>第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)</h2><p>第一周：深度学习的实用层面(Practical aspects of Deep Learning)</p>
<p>1.1 训练，验证，测试集（Train &#x2F; Dev &#x2F; Test sets）</p>
<p>1.2 偏差，方差（Bias &#x2F;Variance）</p>
<p>1.3 机器学习基础（Basic Recipe for Machine Learning）</p>
<p>1.4 正则化（Regularization）</p>
<p>1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）</p>
<p>1.6 dropout 正则化（Dropout Regularization）</p>
<p>1.7 理解 dropout（Understanding Dropout）</p>
<p>1.8 其他正则化方法（Other regularization methods）</p>
<p>1.9 标准化输入（Normalizing inputs）</p>
<p>1.10 梯度消失&#x2F;梯度爆炸（Vanishing &#x2F; Exploding gradients）</p>
<p>1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing &#x2F;Exploding gradients）</p>
<p>1.12 梯度的数值逼近（Numerical approximation of gradients）</p>
<p>1.13 梯度检验（Gradient checking）</p>
<p>1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）</p>
<p>第二周：优化算法 (Optimization algorithms)</p>
<p>2.1 Mini-batch 梯度下降（Mini-batch gradient descent）</p>
<p>2.2 理解Mini-batch 梯度下降（Understanding Mini-batch gradient descent）</p>
<p>2.3 指数加权平均（Exponentially weighted averages）</p>
<p>2.4 理解指数加权平均（Understanding Exponentially weighted averages）</p>
<p>2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted averages）</p>
<p>2.6 momentum梯度下降（Gradient descent with momentum）</p>
<p>2.7 RMSprop——root mean square prop（RMSprop）</p>
<p>2.8 Adam优化算法（Adam optimization algorithm）</p>
<p>2.9 学习率衰减（Learning rate decay）</p>
<p>2.10 局部最优问题（The problem of local optima）</p>
<p>第三周超参数调试，batch正则化和程序框架（Hyperparameter tuning, Batch Normalization and Programming Frameworks)</p>
<p>3.1 调试处理（Tuning process）</p>
<p>3.2 为超参数选择和适合范围（Using an appropriate scale to pick hyperparameters）</p>
<p>3.3 超参数训练的实践：Pandas vs. Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）</p>
<p>3.4 网络中的正则化激活函数（Normalizing activations in a network）</p>
<p>3.5 将 Batch Norm拟合进神经网络（Fitting Batch Norm into a neural network）</p>
<p>3.6 为什么Batch Norm奏效？（Why does Batch Norm work?）</p>
<p>3.7 测试时的Batch Norm（Batch Norm at test time）</p>
<p>3.8 Softmax 回归（Softmax Regression）</p>
<p>3.9 训练一个Softmax 分类器（Training a softmax classifier）</p>
<p>3.10 深度学习框架（Deep learning frameworks）</p>
<p>3.11 TensorFlow（TensorFlow）</p>
<h2 id="第三门课-结构化机器学习项目-Structuring-Machine-Learning-Projects"><a href="#第三门课-结构化机器学习项目-Structuring-Machine-Learning-Projects" class="headerlink" title="第三门课 结构化机器学习项目 (Structuring Machine Learning Projects)"></a>第三门课 结构化机器学习项目 (Structuring Machine Learning Projects)</h2><p>第一周：机器学习策略（1）(ML Strategy (1))</p>
<p>1.1 为什么是ML策略？ (Why ML Strategy)</p>
<p>1.2 正交化(Orthogonalization)</p>
<p>1.3 单一数字评估指标(Single number evaluation metric)</p>
<p>1.4 满足和优化指标 (Satisficing and Optimizing metric)</p>
<p>1.5 训练集、开发集、测试集的划分(Train&#x2F;dev&#x2F;test distributions)</p>
<p>1.6 开发集和测试集的大小 (Size of the dev and test sets)</p>
<p>1.7 什么时候改变开发集&#x2F;测试集和评估指标(When to change dev&#x2F;test sets and metrics)</p>
<p>1.8 为什么是人的表现 (Why human-level performance?)</p>
<p>1.9 可避免偏差(Avoidable bias)</p>
<p>1.10 理解人类的表现 (Understanding human-level performance)</p>
<p>1.11 超过人类的表现(Surpassing human-level performance)</p>
<p>1.12 改善你的模型表现 (Improving your model performance)</p>
<p>第二周：机器学习策略（2）(ML Strategy (2))</p>
<p>2.1 误差分析 (Carrying out error analysis)</p>
<p>2.2 清除标注错误的数据(Cleaning up incorrectly labeled data)</p>
<p>2.3 快速搭建你的第一个系统，并进行迭代(Build your first system quickly, then iterate)</p>
<p>2.4 在不同的分布上的训练集和测试集 (Training and testing on different distributions)</p>
<p>2.5 数据分布不匹配的偏差与方差分析 (Bias and Variance with mismatched data distributions)</p>
<p>2.6 处理数据不匹配问题(Addressing data mismatch)</p>
<p>2.7 迁移学习 (Transfer learning)</p>
<p>2.8 多任务学习(Multi-task learning)</p>
<p>2.9 什么是端到端的深度学习？ (What is end-to-end deep learning?)</p>
<p>2.10 是否使用端到端的深度学习方法 (Whether to use end-to-end deep learning)</p>
<h2 id="第四门课-卷积神经网络（Convolutional-Neural-Networks）"><a href="#第四门课-卷积神经网络（Convolutional-Neural-Networks）" class="headerlink" title="第四门课 卷积神经网络（Convolutional Neural Networks）"></a>第四门课 卷积神经网络（Convolutional Neural Networks）</h2><p>第一周 卷积神经网络(Foundations of Convolutional Neural Networks)</p>
<p>1.1	计算机视觉（Computer vision）</p>
<p>1.2	边缘检测示例（Edge detection example）</p>
<p>1.3	更多边缘检测内容（More edge detection）</p>
<p>1.4	Padding</p>
<p>1.5	卷积步长（Strided convolutions）</p>
<p>1.6	三维卷积（Convolutions over volumes）</p>
<p>1.7	单层卷积网络（One layer of a convolutional network）</p>
<p>1.8	简单卷积网络示例（A simple convolution network example）</p>
<p>1.9	池化层（Pooling layers）</p>
<p>1.10 卷积神经网络示例（Convolutional neural network example）</p>
<p>1.11 为什么使用卷积？（Why convolutions?）</p>
<p>第二周 深度卷积网络：实例探究(Deep convolutional models: case studies)</p>
<p>2.1 为什么要进行实例探究？（Why look at case studies?）</p>
<p>2.2 经典网络（Classic networks）</p>
<p>2.3 残差网络（Residual Networks (ResNets)）</p>
<p>2.4 残差网络为什么有用？（Why ResNets work?）</p>
<p>2.5 网络中的网络以及 1×1 卷积（Network in Network and 1×1 convolutions）</p>
<p>2.6 谷歌 Inception 网络简介（Inception network motivation）</p>
<p>2.7 Inception 网络（Inception network）</p>
<p>2.8 使用开源的实现方案（Using open-source implementations）</p>
<p>2.9 迁移学习（Transfer Learning）</p>
<p>2.10 数据扩充（Data augmentation）</p>
<p>2.11 计算机视觉现状（The state of computer vision）</p>
<p>第三周 目标检测（Object detection）</p>
<p>3.1 目标定位（Object localization）</p>
<p>3.2 特征点检测（Landmark detection）</p>
<p>3.3 目标检测（Object detection）</p>
<p>3.4 卷积的滑动窗口实现（Convolutional implementation of sliding windows）</p>
<p>3.5 Bounding Box预测（Bounding box predictions）</p>
<p>3.6 交并比（Intersection over union）</p>
<p>3.7 非极大值抑制（Non-max suppression）</p>
<p>3.8 Anchor Boxes</p>
<p>3.9 YOLO 算法（Putting it together: YOLO algorithm）</p>
<p>3.10 候选区域（选修）（Region proposals (Optional)）</p>
<p>第四周 特殊应用：人脸识别和神经风格转换（Special applications: Face recognition &amp;Neural style transfer）</p>
<p>4.1 什么是人脸识别？(What is face recognition?)</p>
<p>4.2 One-Shot学习（One-shot learning）</p>
<p>4.3 Siamese 网络（Siamese network）</p>
<p>4.4 Triplet 损失（Triplet 损失）</p>
<p>4.5 面部验证与二分类（Face verification and binary classification）</p>
<p>4.6 什么是神经风格转换？（What is neural style transfer?）</p>
<p>4.7 什么是深度卷积网络？（What are deep ConvNets learning?）</p>
<p>4.8 代价函数（Cost function）</p>
<p>4.9 内容代价函数（Content cost function）</p>
<p>4.10 风格代价函数（Style cost function）</p>
<p>4.11 一维到三维推广（1D and 3D generalizations of models）</p>
<h1 id="第五门课-序列模型-Sequence-Models"><a href="#第五门课-序列模型-Sequence-Models" class="headerlink" title="第五门课 序列模型(Sequence Models)"></a>第五门课 序列模型(Sequence Models)</h1><p>第一周 循环序列模型（Recurrent Neural Networks） 1.1 为什么选择序列模型？（Why Sequence Models?）</p>
<p>1.2 数学符号（Notation）</p>
<p>1.3 循环神经网络模型（Recurrent Neural Network Model）</p>
<p>1.4 通过时间的反向传播（Backpropagation through time）</p>
<p>1.5 不同类型的循环神经网络（Different types of RNNs）</p>
<p>1.6 语言模型和序列生成（Language model and sequence generation）</p>
<p>1.7 对新序列采样（Sampling novel sequences）</p>
<p>1.8 循环神经网络的梯度消失（Vanishing gradients with RNNs）</p>
<p>1.9 GRU单元（Gated Recurrent Unit（GRU））</p>
<p>1.10 长短期记忆（LSTM（long short term memory）unit）</p>
<p>1.11 双向循环神经网络（Bidirectional RNN）</p>
<p>1.12 深层循环神经网络（Deep RNNs）</p>
<p>第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）</p>
<p>2.1 词汇表征（Word Representation）</p>
<p>2.2 使用词嵌入（Using Word Embeddings）</p>
<p>2.3 词嵌入的特性（Properties of Word Embeddings）</p>
<p>2.4 嵌入矩阵（Embedding Matrix）</p>
<p>2.5 学习词嵌入（Learning Word Embeddings）</p>
<p>2.6 Word2Vec</p>
<p>2.7 负采样（Negative Sampling）</p>
<p>2.8 GloVe 词向量（GloVe Word Vectors）</p>
<p>2.9 情绪分类（Sentiment Classification）</p>
<p>2.10 词嵌入除偏（Debiasing Word Embeddings）</p>
<p>第三周 序列模型和注意力机制（Sequence models &amp; Attention mechanism）</p>
<p>3.1 基础模型（Basic Models）</p>
<p>3.2 选择最可能的句子（Picking the most likely sentence）</p>
<p>3.3 集束搜索（Beam Search）</p>
<p>3.4 改进集束搜索（Refinements to Beam Search）</p>
<p>3.5 集束搜索的误差分析（Error analysis in beam search）</p>
<p>3.6 Bleu 得分（选修）（Bleu Score (optional)）</p>
<p>3.7 注意力模型直观理解（Attention Model Intuition）</p>
<p>3.8注意力模型（Attention Model）</p>
<p>3.9语音识别（Speech recognition）</p>
<p>3.10触发字检测（Trigger Word Detection）</p>
<p>3.11结论和致谢（Conclusion and thank you）</p>
<p>人工智能大师访谈</p>
<p>吴恩达采访 Geoffery Hinton</p>
<p>吴恩达采访 Ian Goodfellow</p>
<p>吴恩达采访 Ruslan Salakhutdinov</p>
<p>吴恩达采访 Yoshua Bengio</p>
<p>吴恩达采访 林元庆</p>
<p>吴恩达采访 Pieter Abbeel</p>
<p>吴恩达采访 Andrej Karpathy</p>
<p>附件</p>
<p>深度学习符号指南（原课程翻译）</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://flytowardnewworld.github.io/2019/04/03/deeplearningvideo/" data-id="cl5v7wh6a0025i0vk85dtbd0r" data-title="deeplearningvideo" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-贝叶斯分类器" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/03/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" class="article-date">
  <time class="dt-published" datetime="2019-03-28T00:50:42.000Z" itemprop="datePublished">2019-03-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/03/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/">贝叶斯分类器</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<h1 id="概率论的知识"><a href="#概率论的知识" class="headerlink" title="概率论的知识"></a>概率论的知识</h1>
        
          <p class="article-more-link">
            <a href="/2019/03/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://flytowardnewworld.github.io/2019/03/28/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" data-id="cl5v7whpz00adi0vkhaxzhq33" data-title="贝叶斯分类器" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" rel="tag">贝叶斯分类器</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-二次规划" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/03/25/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92/" class="article-date">
  <time class="dt-published" datetime="2019-03-25T07:28:27.000Z" itemprop="datePublished">2019-03-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>►<a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6/">数学</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/03/25/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92/">二次规划</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<h1 id="KKT-Karush-Kuhn-Tucher-条件"><a href="#KKT-Karush-Kuhn-Tucher-条件" class="headerlink" title="KKT(Karush-Kuhn-Tucher)条件"></a>KKT(Karush-Kuhn-Tucher)条件</h1>
        
          <p class="article-more-link">
            <a href="/2019/03/25/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://flytowardnewworld.github.io/2019/03/25/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92/" data-id="cl5v7wh9w0047i0vkgvsn3eig" data-title="二次规划" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92/" rel="tag">二次规划</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-kaggle" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/03/24/kaggle/" class="article-date">
  <time class="dt-published" datetime="2019-03-24T06:24:46.000Z" itemprop="datePublished">2019-03-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%AB%9E%E8%B5%9B/">竞赛</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/03/24/kaggle/">kaggle</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- more  -->
      
    </div>
    <footer class="article-footer">
      <a data-url="http://flytowardnewworld.github.io/2019/03/24/kaggle/" data-id="cl5v7wh5l001bi0vk65i3d45n" data-title="kaggle" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%AB%9E%E8%B5%9B/" rel="tag">竞赛</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-scikit-learn" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/03/23/scikit-learn/" class="article-date">
  <time class="dt-published" datetime="2019-03-23T10:39:47.000Z" itemprop="datePublished">2019-03-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/03/23/scikit-learn/">scikit-learn</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id=""><a href="#" class="headerlink" title=""></a><!-- more  --></h1><h1 id="Cross-validation-evaluating-estimator-performance¶"><a href="#Cross-validation-evaluating-estimator-performance¶" class="headerlink" title="Cross-validation: evaluating estimator performance¶"></a>Cross-validation: evaluating estimator performance<a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance">¶</a></h1><p><img src="https://scikit-learn.org/stable/_images/grid_search_workflow.png" alt="Grid Search Workflow"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line"># 调用train_test_split函数 自动划分数据集 40%for testing</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target, test_size=0.4, random_state=0)</span><br></pre></td></tr></table></figure>



<h2 id="corss-validation"><a href="#corss-validation" class="headerlink" title="corss validation"></a>corss validation</h2><p><img src="https://scikit-learn.org/stable/_images/grid_search_cross_validation.png" alt="../_images/grid_search_cross_validation.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_validate</span><br><span class="line">from sklearn.metrics import recall_score</span><br><span class="line">scoring = [&#x27;precision_macro&#x27;, &#x27;recall_macro&#x27;]</span><br><span class="line">clf = svm.SVC(kernel=&#x27;linear&#x27;, C=1, random_state=0)</span><br><span class="line">scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,</span><br><span class="line">                        cv=5, return_train_score=False)</span><br><span class="line">sorted(scores.keys())</span><br></pre></td></tr></table></figure>

<h2 id="Cross-validation-of-time-series-data"><a href="#Cross-validation-of-time-series-data" class="headerlink" title="Cross validation of time series data"></a>Cross validation of time series data</h2><p><img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0101.png" alt="../_images/sphx_glr_plot_cv_indices_0101.png"></p>
<h1 id="Tuning-the-hyper-parameters-of-an-estimator"><a href="#Tuning-the-hyper-parameters-of-an-estimator" class="headerlink" title="Tuning the hyper-parameters of an estimator"></a>Tuning the hyper-parameters of an estimator</h1><p>A search consists of:</p>
<ul>
<li><p>an estimator (regressor or classifier such as <code>sklearn.svm.SVC()</code>);</p>
</li>
<li><p>a parameter space;</p>
</li>
<li><p>a method for searching or sampling candidates;</p>
</li>
<li><p>a cross-validation scheme; and</p>
</li>
<li><p>a <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/grid_search.html#gridsearch-scoring">score function</a>.</p>
</li>
</ul>
<h2 id="Grid-Search"><a href="#Grid-Search" class="headerlink" title="Grid Search"></a>Grid Search</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">param_grid = [</span><br><span class="line">  &#123;&#x27;C&#x27;: [1, 10, 100, 1000], &#x27;kernel&#x27;: [&#x27;linear&#x27;]&#125;,</span><br><span class="line">  &#123;&#x27;C&#x27;: [1, 10, 100, 1000], &#x27;gamma&#x27;: [0.001, 0.0001], &#x27;kernel&#x27;: [&#x27;rbf&#x27;]&#125;,</span><br><span class="line"> ]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loading the Digits dataset</span></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line"></span><br><span class="line"><span class="comment"># To apply an classifier on this data, we need to flatten the image, to</span></span><br><span class="line"><span class="comment"># turn the data in a (samples, feature) matrix:</span></span><br><span class="line">n_samples = <span class="built_in">len</span>(digits.images)</span><br><span class="line">X = digits.images.reshape((n_samples, -<span class="number">1</span>))</span><br><span class="line">y = digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the dataset in two equal parts</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the parameters by cross-validation</span></span><br><span class="line">tuned_parameters = [&#123;<span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;rbf&#x27;</span>], <span class="string">&#x27;gamma&#x27;</span>: [<span class="number">1e-3</span>, <span class="number">1e-4</span>],</span><br><span class="line">                     <span class="string">&#x27;C&#x27;</span>: [<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>]&#125;,</span><br><span class="line">                    &#123;<span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;linear&#x27;</span>], <span class="string">&#x27;C&#x27;</span>: [<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>]&#125;]</span><br><span class="line"></span><br><span class="line">scores = [<span class="string">&#x27;precision&#x27;</span>, <span class="string">&#x27;recall&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> score <span class="keyword">in</span> scores:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;# Tuning hyper-parameters for %s&quot;</span> % score)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    clf = GridSearchCV(SVC(), tuned_parameters, cv=<span class="number">5</span>,</span><br><span class="line">                       scoring=<span class="string">&#x27;%s_macro&#x27;</span> % score)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Best parameters set found on development set:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    <span class="built_in">print</span>(clf.best_params_)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Grid scores on development set:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    means = clf.cv_results_[<span class="string">&#x27;mean_test_score&#x27;</span>]</span><br><span class="line">    stds = clf.cv_results_[<span class="string">&#x27;std_test_score&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> mean, std, params <span class="keyword">in</span> <span class="built_in">zip</span>(means, stds, clf.cv_results_[<span class="string">&#x27;params&#x27;</span>]):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%0.3f (+/-%0.03f) for %r&quot;</span></span><br><span class="line">              % (mean, std * <span class="number">2</span>, params))</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Detailed classification report:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The model is trained on the full development set.&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The scores are computed on the full evaluation set.&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    y_true, y_pred = y_test, clf.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(classification_report(y_true, y_pred))</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note the problem is too easy: the hyperparameter plateau is too flat and the</span></span><br><span class="line"><span class="comment"># output model is the same for precision and recall with ties in quality.</span></span><br></pre></td></tr></table></figure>



<h2 id="Randomized-Parameter-Optimization"><a href="#Randomized-Parameter-Optimization" class="headerlink" title="Randomized Parameter Optimization"></a>Randomized Parameter Optimization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint <span class="keyword">as</span> sp_randint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># get some data</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># build a classifier</span></span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Utility function to report best scores</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">report</span>(<span class="params">results, n_top=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_top + <span class="number">1</span>):</span><br><span class="line">        candidates = np.flatnonzero(results[<span class="string">&#x27;rank_test_score&#x27;</span>] == i)</span><br><span class="line">        <span class="keyword">for</span> candidate <span class="keyword">in</span> candidates:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Model with rank: &#123;0&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                  results[<span class="string">&#x27;mean_test_score&#x27;</span>][candidate],</span><br><span class="line">                  results[<span class="string">&#x27;std_test_score&#x27;</span>][candidate]))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Parameters: &#123;0&#125;&quot;</span>.<span class="built_in">format</span>(results[<span class="string">&#x27;params&#x27;</span>][candidate]))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># specify parameters and distributions to sample from</span></span><br><span class="line">param_dist = &#123;<span class="string">&quot;max_depth&quot;</span>: [<span class="number">3</span>, <span class="literal">None</span>],</span><br><span class="line">              <span class="string">&quot;max_features&quot;</span>: sp_randint(<span class="number">1</span>, <span class="number">11</span>),</span><br><span class="line">              <span class="string">&quot;min_samples_split&quot;</span>: sp_randint(<span class="number">2</span>, <span class="number">11</span>),</span><br><span class="line">              <span class="string">&quot;bootstrap&quot;</span>: [<span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">              <span class="string">&quot;criterion&quot;</span>: [<span class="string">&quot;gini&quot;</span>, <span class="string">&quot;entropy&quot;</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># run randomized search</span></span><br><span class="line">n_iter_search = <span class="number">20</span></span><br><span class="line">random_search = RandomizedSearchCV(clf, param_distributions=param_dist,</span><br><span class="line">                                   n_iter=n_iter_search, cv=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">start = time()</span><br><span class="line">random_search.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;RandomizedSearchCV took %.2f seconds for %d candidates&quot;</span></span><br><span class="line">      <span class="string">&quot; parameter settings.&quot;</span> % ((time() - start), n_iter_search))</span><br><span class="line">report(random_search.cv_results_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use a full grid over all parameters</span></span><br><span class="line">param_grid = &#123;<span class="string">&quot;max_depth&quot;</span>: [<span class="number">3</span>, <span class="literal">None</span>],</span><br><span class="line">              <span class="string">&quot;max_features&quot;</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">              <span class="string">&quot;min_samples_split&quot;</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">              <span class="string">&quot;bootstrap&quot;</span>: [<span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">              <span class="string">&quot;criterion&quot;</span>: [<span class="string">&quot;gini&quot;</span>, <span class="string">&quot;entropy&quot;</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># run grid search</span></span><br><span class="line">grid_search = GridSearchCV(clf, param_grid=param_grid, cv=<span class="number">5</span>)</span><br><span class="line">start = time()</span><br><span class="line">grid_search.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;GridSearchCV took %.2f seconds for %d candidate parameter settings.&quot;</span></span><br><span class="line">      % (time() - start, <span class="built_in">len</span>(grid_search.cv_results_[<span class="string">&#x27;params&#x27;</span>])))</span><br><span class="line">report(grid_search.cv_results_)</span><br></pre></td></tr></table></figure>

<p>  step1： 交叉验证（评价模型）</p>
<p>step2: 超参数选择，每一组参数：对应一次交叉验证</p>
<p>step 3: 集成学习</p>
<p>也可进行参数的调解</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.ensemble import AdaBoostClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">clf = AdaBoostClassifier(n_estimators=100)</span><br><span class="line">scores = cross_val_score(clf, iris.data, iris.target, cv=5)</span><br><span class="line">scores.mean()                             </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from itertools import product</span><br><span class="line">from sklearn.ensemble import VotingClassifier</span><br><span class="line"></span><br><span class="line"># Loading some example data</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, [0, 2]]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"># Training classifiers</span><br><span class="line">clf1 = DecisionTreeClassifier(max_depth=4)</span><br><span class="line">clf2 = KNeighborsClassifier(n_neighbors=7)</span><br><span class="line">clf3 = SVC(gamma=&#x27;scale&#x27;, kernel=&#x27;rbf&#x27;, probability=True)</span><br><span class="line">eclf = VotingClassifier(estimators=[(&#x27;dt&#x27;, clf1), (&#x27;knn&#x27;, clf2), (&#x27;svc&#x27;, clf3)],</span><br><span class="line">                        voting=&#x27;soft&#x27;, weights=[2, 1, 2])</span><br><span class="line"></span><br><span class="line">clf1 = clf1.fit(X, y)</span><br><span class="line">clf2 = clf2.fit(X, y)</span><br><span class="line">clf3 = clf3.fit(X, y)</span><br><span class="line">eclf = eclf.fit(X, y)</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://flytowardnewworld.github.io/2019/03/23/scikit-learn/" data-id="cl5v7wh7f0037i0vk0ueu0sdy" data-title="scikit-learn" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Boosting" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/03/22/Boosting/" class="article-date">
  <time class="dt-published" datetime="2019-03-22T06:54:29.000Z" itemprop="datePublished">2019-03-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/03/22/Boosting/">Boosting</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>Boosting算法是将“弱学习算法“提升为“强学习算法”的过程。</p>
<ol>
<li><p>加法模型<br>$$<br>F_n(x;P) &#x3D; \sum_{t&#x3D;1}^{n}\alpha_th_t(x;a_t)<br>$$</p>
</li>
<li><p>前向分步<br>$$<br>F_m(x) &#x3D; F_{m-1}(x)+\alpha_mh_m(x,a_m)<br>$$<br>如果选取不同损失函数，则产生不同的类型</p>
</li>
</ol>
<h1 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h1><p>AdaBoost就是损失函数为指数损失的Boosting算法。</p>
<ol>
<li><p>每一次迭代的弱学习$h(x;a_m)$有何不一样，如何学习？</p>
<p>AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。</p>
</li>
<li><p>弱分类器权值$β_m$如何确定？</p>
<p>AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。这个很好理解，正确率高分得好的弱分类器在强分类器中当然应该有较大的发言权。</p>
</li>
</ol>
<h2 id="原理理解"><a href="#原理理解" class="headerlink" title="原理理解"></a>原理理解</h2><p>基于Boosting的理解，对于AdaBoost，我们要搞清楚两点：</p>
<p>每一次迭代的弱学习h(x;am)有何不一样，如何学习？<br>弱分类器权值βm如何确定？<br>对于第一个问题，AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。然后，再根据所采用的一些基本机器学习算法进行学习，比如逻辑回归。</p>
<p>对于第二个问题，AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。这个很好理解，正确率高分得好的弱分类器在强分类器中当然应该有较大的发言权。</p>
<h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><p>指数损失函数<br>$$<br>L(Y,f(x))&#x3D;exp(-Yf(x))<br>$$<br>权重更新公式: 采用的指数误差函数<br>$$<br>l_{exp}(a_th_t|D_t)&#x3D;E(exp(-f(x)a_th_t(x)))\<br>&#x3D;p(f(x)&#x3D;h_t(x))e^{-at}+p(f(x)!&#x3D;h_t(x))e^{at}\<br>&#x3D;e^{-at}(1-\xi)+e^{at}\xi<br>$$</p>
<p>$$<br>a_t&#x3D;\frac{1}{2}ln \frac{1-\xi}{\xi}<br>$$</p>
<p>分布更新公式<br>$$<br>\begin{aligned} l\left(H_{t-1}(x)+\alpha h_{t}(x) | D\right) &amp;&#x3D;E_{X \sim D}\left(\exp \left(-y(x)\left(H_{t-1}(x)+\alpha h_{t}(x)\right)\right)\right) \ &amp;&#x3D;E_{x \sim D}\left(\exp \left(-y(x) H_{t-1}(x)\right) \exp \left(-y(x) \alpha h_{t}(x)\right)\right) \end{aligned}<br>$$</p>
<p>在泰勒展开$exp(-y(x)h_t(x))$<br>$$<br>\begin{aligned} l\left(H_{t-1}(x)+h_{t}(x) | D\right) &amp; \approx E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\left(1-\alpha y(x) h_{t}(x)+\frac{\alpha^{2} y^{2}(x) h_{t}^{2}(x)}{2}\right)\right] \ &amp;&#x3D;E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\left(1-y(x) h_{t}(x)+0.5 \alpha^{2}\right)\right] \end{aligned}<br>$$</p>
<p>$$<br>\begin{aligned} h(x) &amp;&#x3D;\arg \min <em>{h} l\left(H</em>{t-1}(x)+\alpha h_{t} | D\right) \ &amp;&#x3D;\arg \max <em>{h} E</em>{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right) \alpha y(x) h_{t}(x)\right] \ &amp;&#x3D;\arg \max <em>{h}\left[\frac{\exp \left(-y(x) H</em>{t-1}(x)\right)}{E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right]} y(x) h(x)\right] \end{aligned}<br>$$</p>
<p>$$</p>
<p>$$</p>
<p>令一个新分布,注意分子是常数<br>$$<br>D_{t}(x)&#x3D;\frac{D(x) \exp \left(-y(x) H_{t-1}(x)\right)^{L}}{E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right]}<br>$$</p>
<p>$$<br>\begin{aligned} h(x) &amp;&#x3D;\arg \max <em>{h} E</em>{x \sim D,}(y(x) h(x)) \ &amp;&#x3D;\arg \max <em>{h} E</em>{x \sim D_{t}}(1-2 \mathcal{I}(y(x) \neq h(x))) \ &amp;&#x3D;\arg \min <em>{h} E</em>{x \sim D_{i}}(\mathcal{I}(y(x) \neq h(x))) \end{aligned}<br>$$</p>
<p>同理可得<br>$$<br>\begin{aligned} D_{t+1} &amp;&#x3D;\frac{D(x) \exp \left(-y(x) H_{t}(x)\right)}{E_{x \sim D}\left[\exp \left(-y(x) H_{t}(x)\right)\right]} \ &amp;&#x3D;\frac{D_{t}(x) \cdot E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right] \cdot \exp \left(-y(x) H_{t}(x)\right)}{\exp \left(-y(x) H_{t-1}(x)\right) \cdot E_{x \sim D}\left[\exp \left(-y(x) H_{t}(x)\right)\right]} \ &amp;&#x3D;D_{t}(x) \exp \left(-y(x) \alpha h_{t}(x)\right) \cdot C . \quad(C i s a \text {constant}) \end{aligned}<br>$$</p>
<p>$$<br>Z_{t}&#x3D;\sum_{i}^{m} D_{t}(x) \exp \left(-y(x) \alpha_{t} h_{y}(x)\right)<br>$$</p>
<p>指数误差函数<br>$$<br>\begin{aligned} l(H(x) | D) &amp;&#x3D;\frac{1}{m} \sum_{i}^{m} \exp \left(-y_{i} H\left(x_{i}\right)\right) \ &amp;&#x3D;\frac{1}{m} \sum_{i}^{m} \exp \left(-\sum_{j}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \ &amp;&#x3D;\sum_{i}^{m} D_{1}\left(x_{i}\right) \exp \left(-\sum_{j}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \ &amp;&#x3D;Z_{1} Z_{2}\left(x_{i}\right) \exp \left(-\sum_{j&#x3D;2}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \ &amp; \vdots \ &amp;&#x3D;\prod_{i&#x3D;1}^{T} Z_{i} \end{aligned}<br>$$</p>
<h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>总结一下，得到AdaBoost的算法流程：</p>
<p>输入：训练数据集$T&#x3D;{(x1,y1),(x2,y2),(xN,yN)}T&#x3D;{(x1,y1),(x2,y2),(xN,yN)}$，其中，$xi∈X⊆Rnxi∈X⊆Rn，yi∈Y&#x3D;−1,1yi∈Y&#x3D;−1,1，$迭代次数M</p>
<p>初始化训练样本的权值分布：$D1&#x3D;(w1,1,w1,2,…,w1,i),w,i&#x3D;1,2,…,N$。</p>
<p>对于$m&#x3D;1,2,…,M$</p>
<p>(a)　使用具有权值分布$D_m$的训练数据集进行学习，得到弱分类器$h_m(x)$　(b)　计算$h_m(x)$在训练数据集上的分类误差率：</p>
<p>$e_m&#x3D;∑_{i&#x3D;1}^{N}w_m,iI(h_m(xi)≠y_i)$</p>
<p>(c)　计算$h_m(x)$在强分类器中所占的权重：</p>
<p>$\alpha_m&#x3D;\frac{1}{2}log(\frac{1−e_m}{e_m})$</p>
<p>(d)　更新训练数据集的权值分布（这里，$z_m是归一化因子，为了使样本的概率分布和为1）：</p>
<p>$$w_{m+1,i}&#x3D;\frac{w_{m,i}}exp(−α_my_ih_m(xi))，i&#x3D;1,2,…,10$$</p>
<p>$$z_m&#x3D;∑_{i&#x3D;1}^{N}w_{m,i}exp(−α_my_ih_m(xi))$$</p>
<p> 得到最终分类器：</p>
<p>$$F(x)&#x3D;sign(∑_{i&#x3D;1}^{N}α_mh_m(x))$$</p>
<h2 id="面经"><a href="#面经" class="headerlink" title="面经"></a>面经</h2><p>今年8月开始找工作，参加大厂面试问到的相关问题有如下几点：</p>
<ol>
<li><p>手推AdaBoost</p>
</li>
<li><p>与GBDT比较</p>
</li>
<li><p>AdaBoost几种基本机器学习算法哪个抗噪能力最强，哪个对重采样不敏感？</p>
</li>
</ol>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><h2 id="实例计算"><a href="#实例计算" class="headerlink" title="实例计算"></a>实例计算</h2><h2 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/davidwang456/articles/8927029.html">https://www.cnblogs.com/davidwang456/articles/8927029.html</a></p>
<h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://flytowardnewworld.github.io/2019/03/22/Boosting/" data-id="cl5v7wh4g000ui0vk8kmh1bw4" data-title="Boosting" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Boosting-AdaBoost/" rel="tag">Boosting, AdaBoost</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-SVR" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/03/19/SVR/" class="article-date">
  <time class="dt-published" datetime="2019-03-19T06:34:14.000Z" itemprop="datePublished">2019-03-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/03/19/SVR/">支持向量回归</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<p>支持向量机用于分类:硬间隔和软件间隔支持向量机。尽可能分对</p>
<p>支持向量机回归： 希望$f(x)$与$y$尽可能的接近。</p>
        
          <p class="article-more-link">
            <a href="/2019/03/19/SVR/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://flytowardnewworld.github.io/2019/03/19/SVR/" data-id="cl5v7wh9o0042i0vk7e6z0wn2" data-title="支持向量回归" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%9E%E5%BD%92/" rel="tag">支持向量机回归</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-SVMClassifiar" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/03/17/SVMClassifiar/" class="article-date">
  <time class="dt-published" datetime="2019-03-17T00:50:59.000Z" itemprop="datePublished">2019-03-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/03/17/SVMClassifiar/">支持向量机(SVM) ----- 分类器</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
        
          <p class="article-more-link">
            <a href="/2019/03/17/SVMClassifiar/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://flytowardnewworld.github.io/2019/03/17/SVMClassifiar/" data-id="cl5v7wh90003ji0vk26mz7bxm" data-title="支持向量机(SVM) ----- 分类器" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="tag">支持向量机</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-回归树" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/03/11/%E5%9B%9E%E5%BD%92%E6%A0%91/" class="article-date">
  <time class="dt-published" datetime="2019-03-11T06:36:29.000Z" itemprop="datePublished">2019-03-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/03/11/%E5%9B%9E%E5%BD%92%E6%A0%91/">回归树</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<h1 id="分类树与回归树"><a href="#分类树与回归树" class="headerlink" title="分类树与回归树"></a>分类树与回归树</h1><p>分类树用于分类问题。分类决策树在选取划分点，用信息熵、信息增益、或者信息增益率、或者基尼系数为标准。<br>Classification tree analysis is when the predicted outcome is the class to which the data belongs.</p>
<p>回归决策树用于处理输出为连续型的数据。回归决策树在选取划分点，就希望划分的两个分支的误差越小越好。</p>
<p>Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient’s length of stay in a hospital)。</p>
        
          <p class="article-more-link">
            <a href="/2019/03/11/%E5%9B%9E%E5%BD%92%E6%A0%91/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://flytowardnewworld.github.io/2019/03/11/%E5%9B%9E%E5%BD%92%E6%A0%91/" data-id="cl5v7whbd004yi0vkhey89963" data-title="回归树" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%9B%9E%E5%BD%92%E6%A0%91/" rel="tag">回归树</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/12/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/14/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Blog/">Blog</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Book/">Book</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/R/">R</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A8%B1%E4%B9%90%E7%94%9F%E6%B4%BB/">娱乐生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E3%81%AE%E5%8E%86%E7%A8%8B-Journal-of-Studying/">学习の历程(Journal of Studying)</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BF%83%E5%BE%97/">心得</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/">数据可视化</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">数据挖掘</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6-Data-Science/">数据科学(Data Science)</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%97%A5%E5%BF%97/">日志</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6/">数学</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A7%91%E6%99%AE/">科普</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A7%91%E7%A0%94/">科研</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%9E%E8%B5%9B/">竞赛</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%8B%B1%E8%AF%AD/">英语</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%A7%84%E5%88%92/">规划</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%A7%86%E9%A2%91%E5%AD%A6%E4%B9%A0/">视频学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E6%97%A5%E5%B8%B8/">读书日常</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/1/" rel="tag">1</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BI/" rel="tag">BI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BP/" rel="tag">BP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Boosting-AdaBoost/" rel="tag">Boosting, AdaBoost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Daily/" rel="tag">Daily</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Mining/" rel="tag">Data Mining</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-learning/" rel="tag">Deep learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/English/" rel="tag">English</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Excel/" rel="tag">Excel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Numpuy/" rel="tag">Numpuy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/" rel="tag">Pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/" rel="tag">SQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVD/" rel="tag">SVD</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/" rel="tag">deep learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/origin/" rel="tag">origin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorlow/" rel="tag">tensorlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/test/" rel="tag">test</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wan/" rel="tag">wan</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92/" rel="tag">二次规划</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/" rel="tag">交叉验证</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99/" rel="tag">关联规则</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/" rel="tag">决策树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/" rel="tag">可视化</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/" rel="tag">回归分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E5%BD%92%E6%A0%91/" rel="tag">回归树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E6%8E%92/" rel="tag">安排</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E8%85%8A%E5%AD%97%E6%AF%8D/" rel="tag">希腊字母</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" rel="tag">归一化</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%A9%E9%93%85/" rel="tag">彩铅</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%88%90%E9%95%BF/" rel="tag">成长</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%88%91%E7%9A%84%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="tag">我的读书笔记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E8%83%BD/" rel="tag">技能</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="tag">支持向量机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%9E%E5%BD%92/" rel="tag">支持向量机回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" rel="tag">数据分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%80%E8%83%BD/" rel="tag">数据分析技能</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" rel="tag">数据挖掘</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2/" rel="tag">数据探索</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%B0%E6%A6%82%E5%BF%B5/" rel="tag">新概念</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%B8%B8/" rel="tag">日常</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%87%E5%87%86%E5%8C%96/" rel="tag">标准化</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AD%A3%E5%88%99%E5%8C%96/" rel="tag">正则化</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag">特征工程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%94%9F%E6%B4%BB%E6%97%A5%E5%BF%97/" rel="tag">生活日志</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">监督学习与非监督学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6/" rel="tag">社会科学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%91%E7%A0%94/" rel="tag">科研</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/" rel="tag">科研工具</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%91%E7%A0%94%E7%AC%94%E8%AE%B0/" rel="tag">科研笔记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AB%9E%E8%B5%9B/" rel="tag">竞赛</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%98%E5%9B%BE/" rel="tag">绘图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/" rel="tag">统计学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/" rel="tag">西瓜书</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%84%E5%88%92/" rel="tag">规划</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%BB%E4%B9%A6/" rel="tag">读书</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" rel="tag">贝叶斯分类器</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/1/" style="font-size: 10px;">1</a> <a href="/tags/BI/" style="font-size: 12.5px;">BI</a> <a href="/tags/BP/" style="font-size: 10px;">BP</a> <a href="/tags/Boosting-AdaBoost/" style="font-size: 10px;">Boosting, AdaBoost</a> <a href="/tags/Daily/" style="font-size: 15px;">Daily</a> <a href="/tags/Data-Mining/" style="font-size: 10px;">Data Mining</a> <a href="/tags/Deep-learning/" style="font-size: 17.5px;">Deep learning</a> <a href="/tags/English/" style="font-size: 10px;">English</a> <a href="/tags/Excel/" style="font-size: 10px;">Excel</a> <a href="/tags/Numpuy/" style="font-size: 10px;">Numpuy</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Python/" style="font-size: 17.5px;">Python</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/SQL/" style="font-size: 10px;">SQL</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD</a> <a href="/tags/deep-learning/" style="font-size: 10px;">deep learning</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/machine-learning/" style="font-size: 12.5px;">machine learning</a> <a href="/tags/origin/" style="font-size: 10px;">origin</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/tensorlow/" style="font-size: 10px;">tensorlow</a> <a href="/tags/test/" style="font-size: 10px;">test</a> <a href="/tags/wan/" style="font-size: 10px;">wan</a> <a href="/tags/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92/" style="font-size: 10px;">二次规划</a> <a href="/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/" style="font-size: 10px;">交叉验证</a> <a href="/tags/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99/" style="font-size: 10px;">关联规则</a> <a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/" style="font-size: 15px;">决策树</a> <a href="/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/" style="font-size: 15px;">可视化</a> <a href="/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/" style="font-size: 10px;">回归分析</a> <a href="/tags/%E5%9B%9E%E5%BD%92%E6%A0%91/" style="font-size: 10px;">回归树</a> <a href="/tags/%E5%AE%89%E6%8E%92/" style="font-size: 10px;">安排</a> <a href="/tags/%E5%B8%8C%E8%85%8A%E5%AD%97%E6%AF%8D/" style="font-size: 10px;">希腊字母</a> <a href="/tags/%E5%BD%92%E4%B8%80%E5%8C%96/" style="font-size: 10px;">归一化</a> <a href="/tags/%E5%BD%A9%E9%93%85/" style="font-size: 10px;">彩铅</a> <a href="/tags/%E6%88%90%E9%95%BF/" style="font-size: 10px;">成长</a> <a href="/tags/%E6%88%91%E7%9A%84%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">我的读书笔记</a> <a href="/tags/%E6%8A%80%E8%83%BD/" style="font-size: 10px;">技能</a> <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" style="font-size: 10px;">支持向量机</a> <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">支持向量机回归</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 20px;">数据分析</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%80%E8%83%BD/" style="font-size: 10px;">数据分析技能</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" style="font-size: 10px;">数据挖掘</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2/" style="font-size: 10px;">数据探索</a> <a href="/tags/%E6%96%B0%E6%A6%82%E5%BF%B5/" style="font-size: 10px;">新概念</a> <a href="/tags/%E6%97%A5%E5%B8%B8/" style="font-size: 12.5px;">日常</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%A0%87%E5%87%86%E5%8C%96/" style="font-size: 10px;">标准化</a> <a href="/tags/%E6%AD%A3%E5%88%99%E5%8C%96/" style="font-size: 10px;">正则化</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">深度学习</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" style="font-size: 10px;">特征工程</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E6%97%A5%E5%BF%97/" style="font-size: 10px;">生活日志</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">监督学习与非监督学习</a> <a href="/tags/%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6/" style="font-size: 10px;">社会科学</a> <a href="/tags/%E7%A7%91%E7%A0%94/" style="font-size: 10px;">科研</a> <a href="/tags/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/" style="font-size: 10px;">科研工具</a> <a href="/tags/%E7%A7%91%E7%A0%94%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">科研笔记</a> <a href="/tags/%E7%AB%9E%E8%B5%9B/" style="font-size: 10px;">竞赛</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 10px;">算法</a> <a href="/tags/%E7%BB%98%E5%9B%BE/" style="font-size: 10px;">绘图</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/" style="font-size: 10px;">统计学</a> <a href="/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/" style="font-size: 10px;">西瓜书</a> <a href="/tags/%E8%A7%84%E5%88%92/" style="font-size: 10px;">规划</a> <a href="/tags/%E8%AF%BB%E4%B9%A6/" style="font-size: 10px;">读书</a> <a href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" style="font-size: 10px;">贝叶斯分类器</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">July 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/07/21/%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/FeatureExtraction/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/07/21/recording%20of%20master/MyMain/">心路历程</a>
          </li>
        
          <li>
            <a href="/2022/07/21/%E8%BF%90%E8%90%A5%E4%B9%8B%E7%AB%9E%E5%93%81%E5%88%86%E6%9E%90/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/07/21/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E4%B9%8B%E9%93%B6%E8%A1%8C%E4%B8%9A%E5%8A%A1%E6%A8%A1%E5%9D%97/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/07/21/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E4%B9%8B%E9%93%B6%E8%A1%8C%E4%B8%9A%E5%8A%A1/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>